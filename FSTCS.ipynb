{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd2c26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81b25a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, criterion, epoch, device):\n",
    "    \"\"\" Training a model for one epoch \"\"\"\n",
    "    \n",
    "    loss_list = []\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "         \n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "         \n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_list.append(loss.item())\n",
    "         \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "         \n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "    mean_loss = np.mean(loss_list)\n",
    "    return mean_loss, loss_list\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_model(model, eval_loader, criterion, device):\n",
    "    \"\"\" Evaluating the model for either validation or test \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss_list = []\n",
    "    for images, labels in eval_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "                 \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_list.append(loss.item())\n",
    "            \n",
    "        # Get predictions from the maximum value\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        correct += len( torch.where(preds==labels)[0] )\n",
    "        total += len(labels)\n",
    "                 \n",
    "    # Total correct predictions and loss\n",
    "    accuracy = correct / total * 100\n",
    "    loss = np.mean(loss_list)\n",
    "    \n",
    "    return accuracy, loss\n",
    "\n",
    "\n",
    "def train_model(model, optimizer, scheduler, criterion, train_loader, valid_loader, num_epochs):\n",
    "    \"\"\" Training a model for a given number of epochs\"\"\"\n",
    "    \n",
    "    train_loss = []\n",
    "    val_loss =  []\n",
    "    loss_iters = []\n",
    "    valid_acc = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "           \n",
    "        # validation epoch\n",
    "        model.eval()  # important for dropout and batch norms\n",
    "        accuracy, loss = eval_model(\n",
    "                    model=model, eval_loader=valid_loader,\n",
    "                    criterion=criterion, device=device\n",
    "            )\n",
    "        valid_acc.append(accuracy)\n",
    "        val_loss.append(loss)\n",
    "        \n",
    "        # training epoch\n",
    "        model.train()  # important for dropout and batch norms\n",
    "        mean_loss, cur_loss_iters = train_epoch(\n",
    "                model=model, train_loader=train_loader, optimizer=optimizer,\n",
    "                criterion=criterion, epoch=epoch, device=device\n",
    "            )\n",
    "        scheduler.step()\n",
    "        train_loss.append(mean_loss)\n",
    "        loss_iters = loss_iters + cur_loss_iters\n",
    "        \n",
    "        if(epoch % 5 == 0 or epoch==num_epochs-1):\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "            print(f\"    Train loss: {round(mean_loss, 5)}\")\n",
    "            print(f\"    Valid loss: {round(loss, 5)}\")\n",
    "            print(f\"    Accuracy: {accuracy}%\")\n",
    "            print(\"\\n\")\n",
    "    \n",
    "    print(f\"Training completed\")\n",
    "    return train_loss, val_loss, loss_iters, valid_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eee62ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lock_layers(model,params_dict):\n",
    "    \"\"\"locks all convolutional layers except the classifier one before training\"\"\"\n",
    "    \n",
    "    model_params = params_dict.keys()  \n",
    "    for key in model_params:\n",
    "        if key.startswith('classifier'):\n",
    "            params_dict[key].requires_grad = True\n",
    "        else:  \n",
    "            params_dict[key].requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e588f366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train_test_plot(model,\n",
    "                          title,\n",
    "                          train_loader,\n",
    "                          valid_loader,\n",
    "                          test_loader,\n",
    "                          optimizer,\n",
    "                          scheduler,\n",
    "                          criterion = nn.CrossEntropyLoss(),\n",
    "                          num_epochs = 15,\n",
    "                          flag_fix_layers = False):\n",
    "    \"\"\"\n",
    "    Trains, tests and analysis a model\n",
    "    Params:\n",
    "    ----------\n",
    "    title: str\n",
    "        Title of the model\n",
    "    flag_fix_layers: Bool \n",
    "        Locks classification layers in the model if True\n",
    "    \n",
    "    Return\n",
    "    ----------\n",
    "    accuracy value and plots of the model\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    \n",
    "    if flag_fix_layers: # parameters in the feature extraction (convolution) layers are frozen during training\n",
    "        params_dict = model.state_dict() \n",
    "        lock_layers(model,params_dict) \n",
    "    \n",
    "    # train model\n",
    "    train_loss, val_loss, loss_iters, valid_acc = train_model(\n",
    "            model=model, optimizer=optimizer, scheduler=scheduler, criterion=criterion,\n",
    "            train_loader=train_loader, valid_loader=valid_loader, num_epochs=15\n",
    "    )\n",
    "    # plot model analysis\n",
    "    plot_analysis(loss_iters,train_loss, val_loss,valid_acc,title) #fixed_models_titles[idx]\n",
    "    # Test on testset\n",
    "    print(f'Currently testing: {title}...\\n')\n",
    "    accuracy, _ = eval_model(model, test_loader, criterion, device)\n",
    "    print(f\"Accuracy of model {title} on the testset: {round(np.max(accuracy),2)}%\\n\")\n",
    "    \n",
    "    return accuracy\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
